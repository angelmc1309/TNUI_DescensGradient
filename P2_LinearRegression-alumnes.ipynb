{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimització - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estadística, la **regressió lineal** és un enfocament lineal per modelar la relació entre una resposta escalar (o variable dependent) i una o més variables explicatives (o variables independents).\n",
    "\n",
    "En regressió lineal, les relacions es modelen mitjançant funcions lineals on els paràmetres del model s’estimen a partir de les dades. \n",
    "Aquests models s'anomenen models lineals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suposem que treballem amb un conjunt de dades $ \\{y_{i}, x_{i_1}, \\ldots, x_{i_m} \\}_{i = 1}^{n}$ de $n$ unitats. \n",
    "\n",
    "Un **model de regressió lineal** assumeix que la relació entre la variable dependent $y$ i el vector $p$ dels regressors $x$ és lineal. \n",
    "Així, el model pren la forma:\n",
    "$${\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i_1}+\\cdots +\\beta _{p}x_{i_p},\\qquad i=1,\\ldots ,n,}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovem que autograd estigui instal·lat\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        import autograd\n",
    "    except:\n",
    "        print('No s\\'ha detectat autograd instal·lat')\n",
    "        if input('Vols instalar autograd? [s/n]').strip().lower() == 's':\n",
    "            !pip3 install --user -U autograd\n",
    "            !pip install --user -U autograd\n",
    "        else:\n",
    "            print('Abans de continuar instal·la autograd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install autograd --upgrade\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install autograd --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import elementwise_grad as grad, value_and_grad\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from IPython.display import display\n",
    "    \n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es permet l'ús de cap llibreria o funció que no estigui ja importat, sigui aquí o més abaix en el notebook.\n",
    "\n",
    "Per Kaggle es permet l'ús d'altres llibreries/funcions sempre i quan únicament es facin servir pel processament de dades, i no pel model i/o predicció."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad\n",
    "\n",
    "En la part anterior, donada una funció havíem de definir també la funció que retorna el seu gradient.\n",
    "\n",
    "En aquesta pràctica, però, veurem que mitjançant l'ús de la llibreria **AutoGrad**, això no és necessari.\n",
    "\n",
    "Consideren les següents funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def f2(x):\n",
    "    x, y = x[0], x[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "def f3(x):\n",
    "    x, y, z = x[0], x[1], x[2]\n",
    "    return (100 * (y - x**2)**2 + (1 - x)**2) + (100 * (z - y**2)**2 + (1 - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donada la funció `f1`, podem fàcilment derivar el seu gradient, que és $2x$.\n",
    "\n",
    "Definim-lo manualment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f1_manual(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara, obtinguem la funció gradient a partir d'autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f1 = grad(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem comprovar que **les dues donen el mateix valor de gradient evaluat en un punt $x$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul manual gradient: df1'(x0) = 2046.0\n",
      "Autograd: df1'(x0) = 2046.0\n"
     ]
    }
   ],
   "source": [
    "x0 = np.asarray([1023.0])\n",
    "print('Calcul manual gradient: df1\\'(x0) = {}'.format(grad_f1_manual(x0)[0]))\n",
    "print('Autograd: df1\\'(x0) = {}'.format(grad_f1(x0)[0]))\n",
    "\n",
    "assert np.allclose(grad_f1_manual(x0), grad_f1(x0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També podem utilitzar `value_and_grad` per obtenir directament amb la **imatge de la funció i el seu gradient en un punt $x$**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1046529.]), array([2046.]))\n"
     ]
    }
   ],
   "source": [
    "val_grad_f1 = value_and_grad(f1)\n",
    "print(val_grad_f1(x0))\n",
    "\n",
    "value, gradient = val_grad_f1(x0)\n",
    "assert np.allclose(value, f1(x0))\n",
    "assert np.allclose(gradient, grad_f1(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclús podem aplicar `grad` sobre un altre `grad` per obtenir la 2a derivada (o Hessià en multiples dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VFX+x/H3SW8kpEFoIaEnBBIgIoiISJEiIFWpY0PXtupa1rI/1y66oruuDWxMaEqXorCgrIhICWUSSCB0CC2FEFJImzm/P4gsakJLJneS+b6eh4fMzJ17vkP55ObcU5TWGiGEEHWfi9EFCCGEqBkS+EII4SQk8IUQwklI4AshhJOQwBdCCCchgS+EEE5CAl8IIZyEBL4QQjgJCXwhhHASbkYXcLGQkBAdERFhdBlCCFGrbN26NUtrHXq54xwq8CMiIkhMTDS6DCGEqFWUUoev5Djp0hFCCCchgS+EEE5CAl8IIZyEBL4QQjgJCXwhhHASVQ58pVQzpdRapVSqUmqXUuqx8udfUkodU0rtKP81qOrlCiGEuFbVMSyzDHhSa71NKVUP2KqUWl3+2nta63eqoQ0hhBBVVOUrfK31Ca31tvKv84BUoElVz3s1jmQX8vKyXZRabTXZrBBCVIt/rdnLtiM5dm+nWvvwlVIRQCdgU/lTjyilkpRSXyilAit5z/1KqUSlVGJmZuY1tZt2Ko8vfz7EV1uOXtP7hRDCKNuO5PDemjTWpV1b/l2Nagt8pZQfsBB4XGt9FvgYaAnEASeAqRW9T2s9XWsdr7WODw297MzgCvWJakDXyCD+tSaN/OKya/sAQghRw7TWTPl2NyF+nkzu2cLu7VVL4Cul3Dkf9rO11osAtNantNZWrbUN+BToWh1tVdI+zw1sR1Z+CZ+uO2CvZoQQolqtSc1g86HTPN63Nb6e9l/ppjpG6SjgcyBVa/3uRc83uuiw4cDOqrZ1KZ3CAxnUIYxPfzpARl6RPZsSQogqK7PaeGvlblqE+HLHdc1qpM3quMLvAUwEbvndEMy3lVLJSqkkoDfwRDW0dUlP39qOkjIb73+/195NCSFElSzYms6+jHyeGdAOd9eamRJV5Z8htNbrAVXBS99W9dxXKzLEl3HXhzN70xHu7hFJy1C/mi5BCCEuq7CkjHdXp9GleSC3tm9YY+3WuZm2f+7TGm93V6Z8t9voUoQQokKfrjtIRl4xzw1sx/le8ZpR5wI/xM+TB29uyeqUU2w8kG10OUII8RsZZ4uYtm4/gzqEER8RVKNt17nAB7j3xkgaBXjx+opUbDZtdDlCCHHBe2vSKLXaeObWdjXedp0MfC93V56+tS3Jx3JZajludDlCCAHAnpN5fL3lKBO7RRAR4lvj7dfJwAe4Pa4JMU38eXvlbopKrUaXI4QQvPFtKn6ebvy5TytD2q+zge/ionhhUDTHc4v4fP1Bo8sRQji5H9My+TEtkz/3aU19Hw9DaqizgQ/QvWUw/aIb8tHafTIZSwhhmDKrjdeWp9A82IeJ3ZsbVkedDnyA5wdFUWK1MXVVmtGlCCGc1NwtR9mbkc9zA6PwdHM1rI46H/iRIb6Yukcwb+tRdh3PNbocIYSTyT1Xyrv/2UO3FkE1OsmqInU+8AEe7dOa+t7uvLo8Ba1lmKYQouZ88MNezpwr5f9ui67RSVYVcYrAD/B25y/92rDxwGlW7TpldDlCCCdxMKuAGRsOMbpLU9o3DjC6HOcIfICxXcNp09CPN75NlWGaQoga8fqKFDxcXXiqf1ujSwGcKPDdXF34+5D2HDldKMM0hRB292NaJmtSM3i0T2sa+HsZXQ7gRIEP0KNVCP2jG/Lh2n2cOivDNIUQ9lFqtfHKsl1EBPtwd48Io8u5wKkCH+Bvg6Mps2reWimraQoh7GPmL4fZn1nA3wZHGzoM8/ecLvDDg324r2cki7Ydq5Fd4oUQziU7v5j31qTRs3UIfaIaGF3Obzhd4AM83LsVDf09eWnpLllNUwhRrf6xag/nSqy86ADDMH/PKQPf19ON5wdFkZSey7zEo0aXI4SoIyxHz/B14lHuuiGC1g3rGV3OHzhl4AMMjW1M14gg3lq5mzOFJUaXI4So5Ww2zYtLdxHs68ljfVsbXU6Fqhz4SqlmSqm1SqlUpdQupdRj5c8HKaVWK6X2lv8eWPVyq49SipeHtT8/7Xm1rLMjhKiaBVvTsRw9w/OD2lHPy93ocipUHVf4ZcCTWusooBvwsFIqGngW+F5r3Rr4vvyxQ4lq5M+k7hHM2nhY1tkRQlyz3HOlvLVyN/HNAxneqYnR5VSqyoGvtT6htd5W/nUekAo0AYYB5vLDzMDtVW3LHp7o14ZAHw9e/EZu4Aohrs3U/+whp7CEl4a2d7gbtRer1j58pVQE0AnYBDTUWp+A898UgArHJyml7ldKJSqlEjMzM6uznCsS4O3OswPbsfVwDgu2pdd4+0KI2m3nsVxmbTzMxG7NiWli/Ho5l1Jtga+U8gMWAo9rrc9e6fu01tO11vFa6/jQ0NDqKueqjOzclPjmgUz5Tm7gCiGunM2m+duSnQT5evIXB1kv51KqJfCVUu6cD/vZWutF5U+fUko1Kn+9EZBRHW3Zg4uL4tXbY8g9V8o/Vu0xuhwhRC3xdeJRdpTfqA3wdswbtRerjlE6CvgcSNVav3vRS0sBU/nXJuCbqrZlT1GN/DF1j2DO5iNYjp4xuhwhhIM7XVDCWyt30zUyyKFv1F6sOq7wewATgVuUUjvKfw0CpgD9lFJ7gX7ljx3aE/1aE+rnyQtLkrHKDVwhxCVM+S6VvKIyXh0W49A3ai/mVtUTaK3XA5V92j5VPX9NquflzotDonlkznYSfjnE3T0ijS5JCOGANh88zbzEdB7o1YK2YY43o7YyTjvTtjKDOzSiV5tQpv4njZO5soSyEOK3SspsPL84mSb1vXmsj2POqK2MBP7vKKV4dVgMpVYbLy/bZXQ5QggH8+lPB9iXkc+rt7fHx6PKnSQ1SgK/AuHBPvy5T2u+23mSH3bLHrhCiPOOZBfy/vd7GRgTxi3tGhpdzlWTwK/E5J4taN3Aj/9bsouC4jKjyxFCGExrzQtLknEv3y61NpLAr4SHmwtvjujAsTPnZHE1IQRLdhzjp71ZPDOgLWEBjrFH7dWSwL+E+Iggxl8fzpc/HyQpXcbmC+GsTheU8OryVDqF12f89c2NLueaSeBfxl8HtiPEz5NnFyZTarUZXY4QwgCvrUjh7LlSpozoiKtL7RhzXxEJ/Mvw93LnlWHtSTlxls/XHzS6HCFEDVu/N4tF247xp14ta9WY+4pI4F+BATGN6B/dkPdWp3Ewq8DocoQQNaSwpIxnFyURGeLLI7e0MrqcKpPAv0Kv3h6Dh5sLf12YJOvmC+Ek/rFqD+k553hrZEe83F2NLqfKJPCvUEN/L/42OIrNB08zZ/MRo8sRQtjZ1sM5zNhwiIndmtM1MsjocqqFBP5VGBPfjBtbhTDlu90cP3PO6HKEEHZSXGblrwuTaBzgzV8HtjO6nGojgX8VlFK8OaIDVpvm+cXJaC1dO0LURf/+fh/7MvJ5fXgMfp61a/mES5HAv0rNgnx4ZkBb/rsnkwVbZUtEIeqa5PRcPv5xPyM6N+HmthXuzFprSeBfA1P3CLpGBPHK8hRZUVOIOqS4zMpT8y2E+Hnw99tq5/IJlyKBfw1cXBRvj+pIqdXGs4uSpGtHiDrigx/2sedUHm+O6ECAj+NvWXi1JPCvUUSIL38d0I7/7slkvnTtCFHrJafn8tF/9zOyc9NauRLmlZDArwJT9wi6Rgbx6rIUGbUjRC32a1dOsK8HL94WbXQ5diOBXwUuLop/jOqIVWueWSATsoSord5dncaeU3m8NbJjnezK+VW1BL5S6gulVIZSaudFz72klDr2u43N65zmwb68MDiK9fuymLXpsNHlCCGu0pZDp5m+7gBjuzajd7u6NSrn96rrCn8GMKCC59/TWseV//q2mtpyOOO6htOrTShvfJsqa+0IUYsUFJfx5DwLTQO9eWFw3e3K+VW1BL7Weh1wujrOVRsppXhrZEc8XF14ct4OymQZZSFqhTe+TeVoTiHvjIqtUxOsKmPvPvxHlFJJ5V0+gXZuy1BhAV68ensM246c4eP/7je6HCHEZfyw+xSzNx3hvhsjub5FsNHl1Ah7Bv7HQEsgDjgBTK3oIKXU/UqpRKVUYmZmph3Lsb9hcU0YGtuYf36/F8tR2SFLCEeVlV/MMwuSaBdWj6dubWt0OTXGboGvtT6ltbZqrW3Ap0DXSo6brrWO11rHh4aG2qucGvPqsBga1vPkia93UFgim58L4Wi01jy7MJmzRWX88844PN1q/7LHV8puga+UanTRw+HAzsqOrUsCfNx5Z0wsB7MLeG1FqtHlCCF+56stR1mTeopnbm1LuzB/o8upUdU1LHMu8AvQVimVrpS6F3hbKZWslEoCegNPVEdbtcENLUOY3LMFczYd4T+7ThpdjhCi3L6MfF5ZlkKPVsHc0yPS6HJqXLXcltZaj63g6c+r49y11ZP92/DzviyeWZhEx6b1CQvwMrokIZxacZmVx77ajpe7C++OicOlFm9Gfq1kpq2deLq58v7YThSX2vjLvB1YZRauEIZ6Z9Uedh0/y9ujYmno75wXYBL4dtQy1I+Xh7Znw/5spq2ToZpCGOXHtEw+/ekgE7s1p1903VwY7UpI4NvZ6PimDO7QiHf/k8a2IzlGlyOE08nMK+bJeRbaNPTjhcFRRpdjKAl8O1NK8caIDoQFePHonO3kFpYaXZIQTsNm0zzx9Q7yi0v599jOeLk7zxDMikjg14AAb3f+PbYTp84W8deFsmGKEDXl4x/3s35fFi8NaU/bsHpGl2M4Cfwa0ik8kL8OaMfKXSeZtVFW1RTC3jYfPM3U/+xhaGxj7riumdHlOAQJ/Bp0742R9G4byqvLU9l5LNfocoSos04XlPDYV9sJD/Lh9eExKOV8QzArIoFfg1xcFFPHxBHs58FDs7eRe07684Wobjab5vGvd5BdUMIH4zpTz6vubmhytSTwa1iQrwcfjOvM8TPneHq+RfrzhahmH6zdx7q0TF4a0p6YJgFGl+NQJPAN0KV5IM8ObMd/Uk7x2U8HjS5HiDrj531ZvLcmjeGdmjC2q/Tb/54EvkHuvTGSAe3DmLJyN5sPOu3eMUJUm5O5RTz21XZahfpJv30lJPANopTi7dEdaRbozcNztpFxtsjokoSotYrLrDw4eyvnSqx8PKEzPh51f/eqayGBbyB/L3emTYwnv6iMh2Zvo1S2RhTimry6PIXtR87wj9GxtGog4+0rI4FvsLZh9XhrVEcSD+fwuqyfL8RVW7A1nVkbj/DATS0Y1KHR5d/gxOTnHgcwNLYxO46c4YufDxLbLIDhnZoaXZIQtUJyei4vLE6me4tgnnairQqvlVzhO4jnBrXj+sggnl2YTHK6TMoS4nKy8ot5YGYiwb4e/HtcJ9xcJc4uR/6EHIS7qwsfje9MiJ8n989MJDOv2OiShHBYJWU2Hpq1jeyCEqZNjCfEz9PokmoFCXwHEuznybSJXcgpLOGh2VspKZObuEJU5NXlKWw+dJq3R3WkQ1OZXHWlJPAdTEyTAN4a2ZEth3L4+9JdMhNXiN+ZvekwMzce5oGbWjAsronR5dQq1bWJ+RdKqQyl1M6LngtSSq1WSu0t/z2wOtpyBsPimvDQzS2Zu/kIMzYcMrocIRzGhv1Z/P2bXfRuG8ozA9oZXU6tU11X+DOAAb977lnge611a+D78sfiCj3Vvy39oxvy6vIU/rsnw+hyhDDcwawCHpy1jcgQX94f2wlXJ9yEvKqqJfC11uuA368PMAwwl39tBm6vjrachYuL4r074mgb5s+jc7azLyPP6JKEMEzuuVLuNW/BRcHnpuvq5AqYNdF9a88+/IZa6xMA5b83sGNbdZKvpxufmeLxdHfl7hlbyMqXkTvC+ZSU2Xhw1laOni7k4wldCA/2MbqkapNfkk+CJYE+CX2YvnW63dsz/KatUup+pVSiUioxMzPT6HIcTpP63nxmiiczr5jJCYkUlVqNLkmIGqO15m9LktmwP5s3R3SkW4tgo0uqMpu2sfbgWu5achdh74RhWmLi0JlDeLrZf2ipPWfanlJKNdJan1BKNQIq7IjWWk8HpgPEx8fLkJQKxDWrzz/v6MSDs7fyl3k7+GBsZ1yk/1I4gY/+u595ien8uU9rRnWp3TPQ92bvJcGSQEJSAkdyj1DPox5jY8ZiijPRo1mPGlnd056BvxQwAVPKf//Gjm3VeQNiwnh+YBSvf5vKW4G7eW5QlNElCWFX3+w4xj9W7WFYXGOe6Nva6HKuyZmiM8zbNQ+zxcyGoxtwUS70a9GPKX2mMKzdMHzca7Z7qloCXyk1F7gZCFFKpQN/53zQz1NK3QscAUZXR1vO7L6ekRw5Xci0dQdoFODFXT0ijS5JCLvYsC+Lp+Zb6BoZxNujOtaqte3LbGWs3r8as8XMkt1LKLYWEx0azVt932J8h/E08Tdu7kC1BL7WemwlL/WpjvOL85RSvDS0PafOFvHy8hQa+nsxUFYHFHVM6omzPDBzK5Ehvnw6MR5PN1ejS7oiOzN2Yt5hZlbyLE7mnyTIO4jJnSdjijPRpVEXh/imJatl1jKuLor3x3Zi/GebeOzrHQT7edI1MsjosoSoFsfOnOOuLzfj6+nGjLu7EuDj2MMvMwsymbtzLmaLmW0ntuHm4sbg1oMxxZoY3GYwHq4eRpf4G8qRpu7Hx8frxMREo8uoFXIKShj5yQYy84qZ90B3ohr5G12SEFWSnV/M6Gm/kJlXzPw/daddmGP+my6xlrAibQVmi5kVe1dQZiujc6POmGJNjI0ZS6hvaI3XpJTaqrWOv+xxEvi117Ez5xj18QbKbJqFf7qhTo1PFs4lv7iMcZ9uZM/JPGbee73D/dSqtWbria0kWBKYkzyH7HPZhPmFMaHDBExxJmIaxBha35UGvnTp1GJN6nsz896ujP7kFyZ8vokFD3anQT0vo8sS4qoUl1l5YGYiu46fZfrELg4V9sfzjjMraRZmi5mUzBQ8XT0Z1m4YplgT/Vv2x82ldkVo7apW/EGrBvX48u6ujPt0I5M+38xX93ejvo9j9RsKUZkyq40/z93Oz/uyeXdMLH2iGhpdEudKz7Fk9xLMFjOrD6zGpm10b9qdTwZ/wpj2Ywj0rr3rQErg1wFxzerz6aR47p6xBdMXm5l13/V1cq0RUbfYbJqn5ltYtesUfx8SzYjOxk2s0lrz89GfMe8wMy9lHmeLz9LMvxnP3fgck2In0Sa4jWG1VScJ/DqiR6sQPhrXmT/N2sq9MxIx39MVb4/aMZxNOB+tNS8s2cmSHcd5+ta23G3QnJJDZw6dn/1qSWB/zn583X0ZGT0SU6yJmyNuxkUZvvpMtZLAr0P6RjfkvTvieOyr7dw/M5FPJ8Xj5S6hLxyL1ppXl6cyd/MRHrq5JQ/3blWj7ecV57EgZQFmi5kfD/8IQO+I3vzfTf/HyOiR+Hn41Wg9NUkCv44ZEtuYolIrzyxM4oGZW5k2sYuEvnAYWmve+DaVL34+yN09Inj61rY10q7VZmXtobWYLWYWpS6isLSQ1kGtebX3q0zsOJHm9ZvXSB1Gk8Cvg0bHN8OmNX9dmMyDs7byycQutWa2oqi7tNZMWbmbT386yKTuzXnxtmi7zz7dk7UHs8XMzKSZpJ9NJ8Az4MJQyu5NuzvE7NeaJIFfR91xXThWGzy/OJmHZm3jowmdJfSFYbTWvL1qD9N+PMD468N5eWh7u4Vtzrkcvtr5FWaLmU3HNuGiXBjQagDv9HuHYe2G4eXmvEOXJfDrsHHXh2PTmr8t2cn9CdK9I4yhteb1Fal8tv4gY7uG8+qwmGoP+1JrKav2r8JsMbN0z1JKrCXENIjhnX7vML7jeML8wqq1vdpKAr+Om9CtOe6uimcXJXOveQufTorHx0P+2kXNsNk0Ly/bhfmXw5i6N+elar6yt5y0YLaYmZ08m4yCDEJ8Qngw/kFMsSbiwuKcrsvmcuR/vhO447pw3FxceHqBhbu+3MLnpngZpy/szmo7v1vV3M1Hue/GSF4YHFUtAXwq/xRzkudgtpixnLLg7uLOkLZDMMWaGNhqIO6u8m+7MhL4TmJkl6a4uSr+Ms/C+M82MePurgT5yoxcYR+lVht/mWdhmeU4D93ckqdvbVulsC8uK2ZZ2jLMFjPf7f0Oq7ZyXePr+GDgB9wZcyfBPrV/68OaIIHvRIbFNcHXw42H5mzjjmm/MPPe6wkLcN4bWMI+ikqtPDR7Gz/szuCvA9rx4M0tr+k8Wmu2HN+CeYeZuTvnklOUQ+N6jXnqhqeYFDuJ6NDoaq687pPVMp3QL/uzuc+8hUBfDxLu6UqL0Lo70UTUrNxzpUw2J7Ll8Gleuz2G8ddf/fj29LPpFxYs2521Gy83L0ZEjcAUa6JPZB9cXWTgwe/J8sjikpLSz3DXl1sA+PKu64htVt/gikRtdzK3CNMXmzmQlc/UMXEMjW18xe8tLC1kcepizBYzaw6sQaO5MfxGTLEmRkePJsArwI6V134S+OKyDmYVMOmLTWTllfDxhM7c3LaB0SWJWmpfRh6TPt9M7rlSpk2M58bWIZd9j03b+OnwT5gtZuanzCe/JJ+I+hFM6jiJSbGTaBl0bV1BzkgCX1yRjLwi7vpiC3tO5fH67THc2TXc6JJELbPxQDYPzNyKu6sLM+6+jpgml74a3396//kFy5ISOHTmEH4efoyOHo0p1kTP5j3r3IJlNcFhNkBRSh0C8gArUHYlRYma06CeF18/0I2H52zn2UXJHDldyFP92+LiIuOXxeUt3p7OMwuSCA/y4cu7ula661puUS7zU+ZjtphZf2Q9CkWfFn14tferDG83HF8P3xqu3DnV1Cid3lrrrBpqS1ylel7ufG6K58VvdvLRf/dz+HQhU0fHyqxcUSmtNf/6fi//XLOXbi2CmDYh/g8bjlttVtYcWIPZYmbx7sUUlRXRNrgtb9zyBhM6TqBZQDODqndeMixTAODu6sIbwzvQPNiXKd/tJv10IdMnxdPQX4Ztit86V2Ll6QUWliedYETnJkwZ0REPt/91w6RkpmDeYWZW8iyO5x0n0CuQu+PuxhRromuTrjL71UB278NXSh0EcgANTNNaT//d6/cD9wOEh4d3OXz4sF3rEZf3n10nefzrHdTzcmP6xHgZwSMuOJF7jvsTtrLzeC5/HdCOB25qgVKK7MJs5u6ci9liJvF4Iq7KlYGtB2KKNTGkzRA83TyNLr1Oc5ibtkqpxlrr40qpBsBq4FGt9bqKjpWbto5j98mz3GdOJDOvmNeHd2BUF+O2nxOOIfHQaR6cvY3C4jLeH9uJm9oE8e3ebzFbzCxPW06prZTYhrGYYk2M6zCOhn7G70/rLBwm8H/TmFIvAfla63cqel0C37Fk5xfzyJzt/HIgm4ndmvN/t0X/5kd34Ry01pg3HOK1Fak0DfTmsYFu/PfoAubsnENWYRYNfBswvsN4TLEmYsNijS7XKTnEKB2llC/gorXOK/+6P/CKPdsU1SfYz5OZ93bl7VV7mL7uALuO5/Lh+M40CvA2ujRRQwpLynhh8U7mb0+mSeNEMl2/Z8TCXXi4ejC07VBMsSZubXmrLFhWS9j1Cl8p1QJYXP7QDZijtX69suPlCt9xLU86zjMLkvByd+XdMbEyScsJJB/LZNysD9hfsIIi1+1obFzf5HpMsSbuiLmDIO8go0sU5RyyS+dyJPAd276MfB6Zs43dJ/N46OaW/KVfG9xcpYunLtFa88vRX/j79x/xw+FvsKl8Gvg05t7OJkyxJtqG1MwetOLqOESXjqhbWjXwY8nDPXh52S4++u9+fjmQzb/u6FTpZBtRexzJPcJMy0y+3DGD/Tn7UNqTcJ/e/GPQo4yIvlUWLKsj5ApfXJOlluO8sDgZreGVYe0Z3qmJjK+uZfJL8lmUugizxczag2vRaPxVRzxKevN0TxNP9ovDVWZc1wpyhS/samhsYzqH1+cvX1v4yzwLa1JP8eqwGIL9ZLy1I7NpGz8e+hGzxcyClAUUlBYQWb8FtzR5mD0H4mgR2IJ/3hVHp/BAo0sVdiBX+KJKrDbNtHX7+efqvdTzcuP14R0YECMbRjuavdl7SbAkMDNpJodzD+Pv6c+Y6DH0aDyCWT96sy+zgDuva8bfbovGz1OuA2sbuWkratSek3n8Zd4Odh0/y5DYxrx4WzSh9eRq30hnis4wb9c8zBYzG45uwEW50K9FPybFTmJAyyF8uu4Y09cdINTPkykjO8jIq1pMAl/UuFKrjY//u58PftiHt4crLwyOYnSXptK3X4PKbGWs3r8as8XMkt1LKLYWEx0ajSnWxPgO42ni34QN+7J4fnEyh7ILGd2lKX+7LZoAbxlHX5tJ4AvD7MvI47lFyWw5lEO3FkG8MiyGNg3rGV1WnbYzY+eFBctO5p8kyDuIcTHjMMWZ6NKoC0opMvOKefO7VBZtO0bzYB/eHN6BG1pdfqMS4fgk8IWhbDbNV1uO8vaq3eQXlXHXDRE81rc19bzkSrK6ZBZkXliwbNuJbbi5uDG49WBMsSYGtxmMh6sHAGVWGwm/HOa91WkUl9mYfFMkj97SWpa/rkMk8IVDOF1Qwj9W7earLUcJ9vXkyf5tGBPfTIb7XaMSawkr0lZgtphZsXcFZbYyOjfqjCnWxNiYsYT6hv7m+LV7MnhjRSp7M/K5qU0oLw2Jlk3r6yAJfOFQLEfP8NqKFLYcyqFdWD2eGxTFTa1DpH//Cmit2XpiKzN2zGDuzrmcPneaML8wJnSYgCnOREyDmD+8J/XEWd78bjfr0jKJCPbhuUFR9I9uKH/edZRzBf7WxyFnR/UXJKqV5vwV/5HThRSXWqnn5U54kA/1vGQYYEWKy4o5VXCKkwWnKCwpxEUpQnxCaOgXRpBXYIXhfa7USnrOObLzi3F1UTQN9KGhvxfyA1UtEBgHXf55TW+ViVfC4Sgg2NeDQB93Ms4Wc+zMOXYdz6W+jwdN6ntL8ANWbSOrMIuT+SfJKcoBDf5e/rQJbkMD31DcXCq+8P+JAAAZX0lEQVT+MyoqtXLsTBGZ+cW4AI3re9M4wBs3V0l68T9143/YNX5XFMZwAcIA/5IyZmw4xKfrDpBTWMqNrUJ45JZWXB8Z5FRdD1prfj76M+YdZualzONs8VnCA8KZ1PFhJsVOonVw60rfuy8jj4/W7ucby3FcXRTjuobzUO+WNKgnW1OKP6obXTqiVisoLmP2psNMX3eQrPxiOjYN4L6eLRgUE1anV+M8dOYQCZYEEiwJ7M/Zj6+7L6OiR2GKNdErohcuquLPrrXmlwPZfPbTQX7YnYG3uyvjrw/n/pta0ED2IHZKztWHL+qEolIrC7am8/n6gxzMKqBxgBfjuzVnTHyzOjNrN684jwUpCzBbzPx4+EcAekf05q64uxgRNQI/j8pH0BQUl/HNjuPM2niYlBNnCfb1YGL35kzs1lzWMHJyEvii1rLZNN/vzmDGhoP8vC8bd1fFre3DGBPfjB6tQmrdkE6rzcraQ2sxW8wsSl1EYWkhrYJaYYo1MbHjRJrXb17pe7XW7Dx2lvlbj7J42zHyistoF1YP0w0RDO/URMbSC0Bu2opazMVF0S+6If2iG7I/M5/ZG4+wcFs6y5NOEObvxfDOTRjSsTFRjeo5dF//nqw9mC1mZibNJP1sOgGeAReGUnZv2v2StafnFLIi6QSLth1jz6k8PNxcGBgTxsRuzenSvOIROkJcjlzhi1qhqNTK96kZLNyWzo9pmVhtmhahvgzu0Ih+0Q2JaRyAiwNc+Z8+d5qvd36N2WJm07FNuCgXBrQagCnWxNC2Q/Fyq7yP/WBWAWtSTrEi+QQ7jp4BIK5ZfUbHN+W2Do0J8JFZyqJi0qUj6qzs/GJW7jrJiqQTbDyQjU1Dg3qe9IlqQM/WoXRvEUygr0eN1VNqLWXV/lWYLWaW7llKibWEDg06YIo1Ma7DOBrVa1Th+wpLythyKIef92WxJvUUBzILAGjf2J/BHRsxuEMjmgf71tjnELWXwwS+UmoA8C/AFfhMaz2lsmMl8MXVOl1QwtrdGfywO4Mf0zLJLy5DKYgK86drZBCdmwfSObw+Tep7V3s3iOWkBbPFzOzk2WQUZBDiE8L4DuOZFDuJTmGd/tDe6YISth3OYduRHBIP5bD9aA6lVo2HqwvXtwiiT7sG9IlqSLMg2TJSXB2HCHyllCuQBvQD0oEtwFitdUpFx0vgi6ootdpISs/l531ZbNifheVoLudKrQAE+XoQ3cif6Mb+tGrgR4sQX1qE+hHo435V3whO5Z9iTvIczBYzllMW3F3cGdJ2CKZYEwNbDcTd1Z28olIOZRVyICuffRn5pJ44S8rxsxzPLQLAzUUR3dif7i2C6dEqhOsigvD2kJuv4to5SuB3B17SWt9a/vg5AK31mxUdL4EvqlOZ1cbuk3lsO5LDrmNnSTlxlj2n8igps104xsfDlUYBXjSu702onyf1fc7PBK7n5Ya3h2v5KJhSNp9czZrD80g8tRabthLp35HrGgyndb1bKS72IbughONnznEit4jcc6UXzu+ioGWoH9GN/Ylu5E9cs/p0bFpfAl5UK0cZpdMEOHrR43Tg+osPUErdD9wPEB4ebudyhDNxc3UhpkkAMU0CLjxXZrWRnnOOg1kF7M/M59iZc5zMLeJ4bhEHMgs4U1hCQYkVjaZEpZHv9j2FruuwqXxcdRB+ZcPxtd6C7Vw4m07BDrczBPkWEuTrQdNAb66LCKJRfa8LP0GEB/nI0EnhMOwd+BX9rPybHym01tOB6XD+Ct/O9Qgn5+bqQkSILxEhvvRu98ct/Y7mHmXGjgQSkhI4cjoNLzdvBkXcxu1txtKtyc14uLrh5e6Kt4crvh5ucqUuahV7B3460Oyix02B43ZuU4irUlBSwOLdi5mxYwY/HPwBjaZneE+e7fEMo9uPxt/T3+gShagW9g78LUBrpVQkcAy4Exhn5zaFuCybtvHT4Z8wW8zMT5lPfkk+kfUjebHXi0yKnUSLwBZGlyhEtbNr4Guty5RSjwCrOD8s8wut9S57tinEpew/vZ8ESwIzk2Zy8MxB/Dz8GBM9BlOciRvDb6x0wTIh6gK7L62gtf4W+Nbe7QhRmdyiXOanzMdsMbP+yHoUij4t+vBK71cY3m44vh4yuUk4B1lLR9RJVpuVNQfWYLaYWbx7MUVlRbQNbssbt7zBhI4TaBbQ7PInEaKOkcAXdUpKZgrmHWZmJc/ieN5xAr0CuTvubkyxJro26SqLjgmnJoEvar3swmzm7pyL2WIm8XgirsqVga0H8q8B/2JImyF4usla8UKABL6opUqtpXy791vMFjPL05ZTaisltmEs7/Z/l3EdxtHQr6HRJQrhcCTwRa2htWb7ye2Yd5iZs3MOWYVZNPBtwCNdH8EUayI2LNboEoVwaBL4wuGdyDvB7OTZmC1mdmbsxMPVg6Fth2KKNXFry1txd5V14oW4EhL4wiEVlRXxze5vMFvMrNq/Cpu20a1pNz4e/DFj2o8hyDvI6BKFqHUk8IXD0FrzS/ovmHeY+XrX1+QW59LMvxnP9niWSbGTaBvS1ugShajVJPCF4Q6fOczMpJkkWBLYe3ovPu4+jIwaiSnWRO/I3jL7VQBQWlpKeno6RUVFRpdiGC8vL5o2bYq7+7V1Y0rgC0Pkl+SzMGUhZouZtYfWAnBzxM083/N5RkaNpJ5nPYMrFI4mPT2devXqERER4ZTzKbTWZGdnk56eTmRk5DWdQwJf1BibtvHjoR8xW8wsSFlAQWkBLQNb8srNrzAxdiIR9SOMLlE4sKKiIqcNewClFMHBwWRmZl7zOSTwhd3tzd57YcGyw7mH8ff0Z2zMWO6Ku4sbmt3gtP+BxdVz9n8rVf38EvjCLs4UnWHernmYLWY2HN2Ai3KhX4t+vNnnTW5vdzve7t5GlyiE05HAF9WmzFbG6v2rMVvMLNm9hGJrMdGh0bzV9y0mdJxA43qNjS5RiCp5//33+fjjj+ncuTOjR48mKSmJF198sdLjn3rqKQYNGsQtt9xSg1VWTgJfVNnOjJ0XFiw7mX+SYO9gJneejCnORJdGXZz+x3BRd3z00Ud89913REZGcsMNN7B06dJLHv/oo48yefJkCXxRu2UWZDIneQ4JSQlsO7ENNxc3BrcejCnWxOA2g/Fw9TC6RFGHvbxsFynHz1brOaMb+/P3Ie0rff1Pf/oTBw4cYOjQoUyYMAFPT09CQkIAGDZsGCNHjmTSpElMmzaNdevWMXv2bJo3b052djYnT54kLCysWuu9FhL44oqVWEtYnrYcs8XMt3u/pcxWRudGnfnXgH8xNmYsob6hRpcohN188sknrFy5krVr17Js2TI6d+584bXp06fTo0cPIiMjmTp1Khs3brzwWufOnfn5558ZOXKkEWX/hgS+uCStNYnHEzFbzMzdOZfT504T5hfG49c/jinOREyDGKNLFE7oUlfiNeHEiROEhv7vAqdhw4a88sor9O7dm8WLFxMU9L+lPxo0aMDx48eNKPMPJPBFhY6dPcaspFkkJCWQkpmCp6snt7e7HVOsiX4t++HmIv90hPPy9vYmNzf3N88lJycTHBz8h3AvKirC29sxRqXZ7X+tUuolYDLw6yyB58v3txUOqrC0kCW7l2C2mFlzYA02beOGZjcw7bZpjGk/hvpe9Y0uUQiHEBUVxaxZsy483rx5M9999x3bt2+nV69e9O/f/8Js2LS0NEaPHm1Uqb9h78u097TW79i5DVEFWmvWH1mP2WJm3q555JXkER4QzvM3Ps+k2Em0Dm5tdIlCOJybbrqJJ598Eq01JSUlTJ48mS+//JLGjRszdepU7rnnHn744QfKysrYt28f8fHxRpcMSJeO0zp05hAJlgQSLAnsz9mPr7svo6JHYYo10SuilyxYJkQFDh06dOHrvn378v3339O3b18sFsuF54cOHcrQoUMBWL58OaNGjcLNzTGi1t5VPKKUmgQkAk9qrXPs3J64hLPFZ1mQsoAESwI/Hv4RhaJ3ZG9e7PUiI6JG4OfhZ3SJQtQazz//PJs2bbrkMWVlZTz55JM1VNHlKa31tb9ZqTVARYNLXwA2AlmABl4FGmmt76ngHPcD9wOEh4d3OXz48DXXI/7IarPyw8EfMFvMLEpdxLmyc7QOao0p1sTE2ImEB4QbXaIQVyQ1NZWoqCijyzBcRX8OSqmtWuvL9htV6Qpfa933So5TSn0KLK/kHNOB6QDx8fHX/t1H/MaerD2YLWZmJs0k/Ww6AZ4BTIqdhCnWRLem3WT2qxBOyJ6jdBpprU+UPxwO7LRXW+K80+dO8/XOrzFbzGw6tglX5cqtrW7lnX7vMKzdMLzcvIwuUQhhIHv24b+tlIrjfJfOIeABO7bltEqtpazctxKzxcyytGWUWEvo0KAD7/R7h/EdxxPmZ/x0biGEY7Bb4GutJ9rr3AIsJy2YLWZmJ88moyCDUJ9QHox/EFOsibiwOOmyEUL8gWOMFRJX5FT+KWYnzybBkoDllAV3F3eGtB2CKdbEwFYDcXe9tn0uhRDX5qWXXsLPz4/bbruNO++8E6UUCxYsoHHjxgwYMIAffvgBV1fXCt+bnJzM1KlTmTFjRo3VK4Hv4IrLilmWtowZO2awct9KrNrKdY2v44OBH3BnzJ0E+wQbXaIQTm/JkiUMGzaMl19+GYAPP/yQESNGVBr2AB06dCA9PZ0jR44QHl4zo+Uk8B2Q1prNxzZjtpj5audX5BTl0LheY5664SlMsSaiQmVomnBuj698nB0nd1TrOePC4vjngH9e9rjXX3+dhIQEmjVrRmhoKFFRUXz00Ue4urqybt061q5dy+zZs5kzZw4Aixcv5sMPP2T16tWcPHmSXr16sW7dOsLCwhgyZAhfffUVzzzzTLV+lspI4DuQo7lHmZk0kwRLAnuy9+Dt5s3wqOGYYk30ieyDq0vlVwtCCPvbunUrX331Fdu3b6esrIzOnTvTpUsX/vSnP+Hn58dTTz1FSUkJBw4cICIiAoDhw4ezcOFCPvzwQ1auXMnLL798YW38+Ph4pkyZIoHvLApKCliUuoiEpAS+P/A9Gk3P8J48fcPTjG4/Gn9Pf6NLFMLhXMmVuD389NNPDB8+HB8fH4ALSyhcLCsri/r1f7vQ4L///W9iYmLo1q0bY8eOvfB8TS+dLIFvAJu2se7wOswWMwtSFpBfkk9k/Uhe7PUik2In0SKwhdElCiEqcbkRcN7e3hQVFf3muWPHjuHi4sKpU6ew2Wy4uJxfq6qml06WFbJq0P7T+/n72r/T8v2W9Db3ZmHKQsZEj+HHu35k35/38dLNL0nYC+HAbrrpJhYvXsy5c+fIy8tj2bJlfzgmMDAQq9V6IfTLysq4++67mTNnDlFRUbz77rsXjk1LSyMmpuY2EZIrfDvLLcplfsp8zBYz64+sR6Ho26Ivr/V+jeFRw/Fx9zG6RCHEFercuTN33HEHcXFxNG/enJ49e1Z4XP/+/Vm/fj19+/bljTfeoGfPnvTs2ZO4uDiuu+46Bg8eTFRUFGvXrmXw4ME1Vn+VFk+rbvHx8ToxMdHoMqrMarOy+sBqEiwJLN69mKKyItqFtMMUa2JCxwk09W9qdIlC1Dq1afG07du38+677zJz5sxKjykuLqZXr16sX7/+qpZPNmzxNPFbKZkpmHeYmZU8i+N5xwn0CuSeuHswxZm4rvF1MvtVCCfRqVMnevfujdVqrXQs/pEjR5gyZUqNrpUvgV9F2YXZzN05F7PFTOLxRFyVK4NaD+L9Ae9zW5vb8HTzNLpEIYQB7rnnD6vB/0br1q1p3bpmd5STwL8GJdYSvtv7HWaLmeVpyym1lRIXFsd7t77HuA7jaODbwOgShRDiDyTwr5DWmu0nt2PeYWbOzjlkFWbRwLcBj3Z9FFOciY4NOxpdohBCXJIE/mWcyDvB7OTZmC1mdmbsxMPVg6Fth2KKNXFry1tlwTIhRK0hgV+BorIivtn9DWaLmVX7V2HTNro17cZHgz7ijpg7CPIOMrpEIYS4ahL45bTW/JL+C+YdZr7e9TW5xbk09W/Ksz2eZVLsJNqGtDW6RCGEg/l1eeRTp06xbNkyPDw8aNmyJV9++eWF5RXGjh3Lrl27uPvuu3niiSd4/PHHGTFiBDfddFOl5+3bty/z588nMDCwWut1+pm2h88c5rV1r9H2g7b0+KIHs5JnMaTtEFZPXM2hxw7xep/XJeyFEJfUr18/du7cSVJSEm3atOHNN98E4OTJk2zYsIGkpCSeeOIJTp8+zcaNGy8Z9gATJ07ko48+qvY6nfIKP78kn4UpCzFbzKw9tBaAXs178dyNzzEqehT1POsZXKEQ4pK2Pg451bs8MoFx0OXql0fu0qUL/fv3v/B6t27dWLBgAXB+xm1GRgZxcXH8+9//JjU1lQEDBgCQm5tL165dWbp0KW3btmXs2LHccsstTJ48maFDh9KzZ09eeOGFav2IThP4Nm1j7cG1mC1mFqYupLC0kJaBLXnl5leYGDuRiPoRRpcohHBwlS2PfLEvvviCO+64A4ClS5dy2223sWPH+W9On332GaNGjQIgICCADz74gLvuuovHHnuMnJwcJk+eDJxfj6e4uJjs7GyCg6tvk6MqBb5SajTwEhAFdNVaJ1702nPAvYAV+LPWelVV2rpWadlpmHeYmZk0k6Nnj+Lv6c/4DuMxxZq4odkNMvtViNroCq7E7eFyyyO//vrruLm5MX78+Arff+LECUJDQy887tevH/Pnz+fhhx/GYrH85thfl052mMAHdgIjgGkXP6mUigbuBNoDjYE1Sqk2WmtrFdu7Ijnncvh619eYLWY2pm/ERbnQv2V/3u73NsPaDsPbveaWIxVC1C2VXSSazWaWL1/O999/X+kxv1862WazkZqaire3N6dPn6Zp0/+ts2WPpZOrdNNWa52qtd5TwUvDgK+01sVa64PAPqBrVdq6nDJbGSvSVjBm/hgaTW3EgyseJK84j7f7vs3RJ47y3fjvuDPmTgl7IcQ1q2x55JUrV/LWW2+xdOnSC1f/FYmKimLfvn0XHr/33ntERUUxd+5c7rnnHkpLS4HzowZPnjx5Ydes6mKvPvwmwMaLHqeXP2cXy9OWc9/S+zhVcIpg72Amd56MKc5El0ZdpMtGCFFtKlse+ZFHHqG4uJh+/foB52/cfvLJJ394/+DBg5k2bRr33XcfaWlpfPbZZ2zevJl69epx00038dprr/Hyyy+zdetWunXrVu0Lq112eWSl1BogrIKXXtBaf1N+zH+Bp37tw1dKfQj8orWeVf74c+BbrfXCCs5/P3A/QHh4eJfDhw9f9YfYlbGLv639G6ZYE4NaD8LD1eOqzyGEcGy1aXnkS7nxxhtZvnz5H7ZBvNhjjz3G0KFD6dOnzx9es+vyyFrrvpc7pgLpQLOLHjcFKty4UWs9HZgO59fDv4a2aN+gPYvvWHwtbxVCiBo1depUjhw5csnAj4mJqTDsq8peXTpLgTlKqXc5f9O2NbDZTm0JIUStcf3111/2mF+HZ1a3Kt20VUoNV0qlA92BFUqpVQBa613APCAFWAk8XFMjdIQQdZcj7dBnhKp+/ipd4WutFwMV9qVorV8HXq/K+YUQ4ldeXl4XJiI542AMrTXZ2dl4eXld8zmcZqatEKJ2a9q0Kenp6WRmZhpdimG8vLx+M1b/akngCyFqBXd3dyIjI40uo1Zz+tUyhRDCWUjgCyGEk5DAF0IIJ3HZmbY1SSmVCVz9VFvjhQBZRhdRw+Qz133O9nmh9n7m5lrr0Msd5FCBX1sppRKvZFpzXSKfue5zts8Ldf8zS5eOEEI4CQl8IYRwEhL41WO60QUYQD5z3edsnxfq+GeWPnwhhHAScoUvhBBOQgK/mimlnlJKaaVUiNG12JNS6h9Kqd1KqSSl1GKlVOWLe9dySqkBSqk9Sql9Sqlnja7H3pRSzZRSa5VSqUqpXUqpx4yuqaYopVyVUtuVUsuNrsUeJPCrkVKqGdAPOGJ0LTVgNRCjte4IpAHPGVyPXSilXIEPgYFANDBWKRVtbFV2VwY8qbWOAroBDzvBZ/7VY0Cq0UXYiwR+9XoPeAao8zdGtNb/0VqXlT/cyPldzeqirsA+rfUBrXUJ8BUwzOCa7EprfUJrva386zzOB6Dd9qR2FEqppsBg4DOja7EXCfxqopQaChzTWluMrsUA9wDfGV2EnTQBjl70OB0nCL9fKaUigE7AJmMrqRH/5PwFm83oQuxFlke+Cpfa0B14HuhfsxXZ1xVuYP8C57sAZtdkbTWoop026vxPcABKKT9gIfC41vqs0fXYk1LqNiBDa71VKXWz0fXYiwT+VahsQ3elVAcgErCU78TTFNimlOqqtT5ZgyVWq8ttYK+UMgG3AX103R3fmw40u+hxU+C4QbXUGKWUO+fDfrbWepHR9dSAHsBQpdQgwAvwV0rN0lpPMLiuaiXj8O1AKXUIiNda18ZFmK6IUmoA8C7QS2tdZ7cgUkq5cf6mdB/gGLAFGFe+b3OdpM5ftZiB01rrx42up6aVX+E/pbW+zehaqpv04Ytr9QFQD1itlNqhlPrE6ILsofzG9CPAKs7fvJxXl8O+XA9gInBL+d/tjvIrX1HLyRW+EEI4CbnCF0IIJyGBL4QQTkICXwghnIQEvhBCOAkJfCGEcBIS+EII4SQk8IUQwklI4AshhJP4f0Mz/MqEOKM4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_f1 = grad(f1) # df(x) = 2 * x\n",
    "grad2_f1 = grad(grad_f1) # d2f(x) = 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    plt.plot(x, f1(x), label = 'f(x)')\n",
    "    plt.plot(x, grad_f1(x), color='green', label = 'df(x)')\n",
    "    plt.plot(x, grad2_f1(x), color='orange', label = 'd2f(x)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descens del gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El primer pas serà implementar la mateixa funció `gradient_descend` que havieu fet en la part anterior, però ara mitjançant l'ús de la llibreria `autograd` i fent-la multidimensional.**\n",
    "\n",
    "Aquesta funció hauria d'executar sense problemes, i donar el mínim, per les tres funcions `f1`, `f2` i `f3` definides més adalt, de 1, 2 i 3 dimensions respectivament. Però, també haurà de funcionar per les següents parts de la pràctica, amb 100 o més dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per utilitzar aquesta llibreria cal tenir instalar el paquet tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "def human_format(num):\n",
    "    \"\"\"\n",
    "    Funció auxiliar per formatar els nombres\n",
    "\n",
    "    100    -> 100.00\n",
    "    2100   -> 2.10K\n",
    "    342100 -> 342.10K\n",
    "    etc.    \n",
    "    \n",
    "    :param num: Nombre a formatar\n",
    "    :return: String amb el nombre formatat\n",
    "    \"\"\"\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.2f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
    "\n",
    "def gradient_descend(f, x0, alpha, eps=1e-6, max_iter=1000, print_iters=1000):\n",
    "    \"\"\"\n",
    "    Aquesta funció implementa l'algorisme de descens pel gradient, és a dir,\n",
    "    donat un punt inicial, la funció de la que calculem el gradient i el pas, \n",
    "    intenta trobar el mínim de la funció seguint el gradient en direcció oposada.\n",
    "    \n",
    "    Pel criteri d'aturada, considerarem si ||x^{k+1} - x^k|| < eps, és a dir\n",
    "    si la norma de la diferència és més petita que eps.\n",
    "    \n",
    "    Cada print_iters cal mostrar el resultat actual en la següent forma:\n",
    "        \"{}/{:.2f}\".format(human_format(itr), float(valor_de_f_en_x))\n",
    "    És dir, un missatge que inclogui la iteració i el valor de f en el punt\n",
    "    \n",
    "    :param f: Funció a minimitzar\n",
    "    :param x0: Punt inicial\n",
    "    :param alpha: Pas de cada iteració\n",
    "    :param eps: Moviment mínim realitzat abans de parar\n",
    "    :param max_epochs: Iteracions màximes a realitzar\n",
    "    :param print_iters: Numero d'iteracions per printar resultats\n",
    "    :return: La funció retornarà el punt on es troba el mínim\n",
    "    \"\"\"\n",
    "       \n",
    "    i=0\n",
    "    val_grad_f = value_and_grad(f)\n",
    "    while i<max_iter:\n",
    "        val_grad=val_grad_f(x0)\n",
    "        #Normalizamos el gradiente para evitar overflow\n",
    "        aux=np.dot(val_grad[1]/ np.linalg.norm(val_grad[1]),alpha)\n",
    "        if np.linalg.norm(aux) < eps:\n",
    "             return x0\n",
    "        else:\n",
    "            x0-=aux\n",
    "            i+=1\n",
    "        if (i % print_iters) == 0:\n",
    "            print(\"{}/{:.2f}\".format(human_format(i), float(val_grad[0])))\n",
    "            \n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquem el mínim de cadascuna de les funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1([4.98237376e-05]) = [2.48240483e-09]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0])\n",
    "    xm = gradient_descend(f1, x0, 0.01, 1e-6, 10000) # ~ @1s\n",
    "    print(f'f1({xm}) = {f1(xm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2([3.06804779e-05 3.91176093e-05]) = 2.4714790816465446e-09\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0, 5.1])\n",
    "    xm = gradient_descend(f2, x0, 0.01, 1e-6, 10000) # ~ @1s\n",
    "    print(f'f2({xm}) = {f2(xm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00K/1.66\n",
      "2.00K/1.49\n",
      "3.00K/1.32\n",
      "4.00K/1.15\n",
      "5.00K/0.98\n",
      "6.00K/0.82\n",
      "7.00K/0.66\n",
      "8.00K/0.51\n",
      "9.00K/0.37\n",
      "10.00K/0.25\n",
      "11.00K/0.16\n",
      "12.00K/0.09\n",
      "13.00K/0.05\n",
      "14.00K/0.03\n",
      "15.00K/0.02\n",
      "16.00K/0.02\n",
      "17.00K/0.02\n",
      "18.00K/0.02\n",
      "19.00K/0.02\n",
      "20.00K/0.02\n",
      "21.00K/0.02\n",
      "22.00K/0.02\n",
      "23.00K/0.02\n",
      "24.00K/0.02\n",
      "25.00K/0.02\n",
      "26.00K/0.02\n",
      "27.00K/0.02\n",
      "28.00K/0.02\n",
      "29.00K/0.02\n",
      "30.00K/0.02\n",
      "31.00K/0.02\n",
      "32.00K/0.02\n",
      "33.00K/0.02\n",
      "34.00K/0.02\n",
      "35.00K/0.02\n",
      "36.00K/0.02\n",
      "37.00K/0.02\n",
      "38.00K/0.02\n",
      "39.00K/0.02\n",
      "40.00K/0.02\n",
      "41.00K/0.02\n",
      "42.00K/0.02\n",
      "43.00K/0.02\n",
      "44.00K/0.02\n",
      "45.00K/0.02\n",
      "46.00K/0.02\n",
      "47.00K/0.02\n",
      "48.00K/0.02\n",
      "49.00K/0.02\n",
      "50.00K/0.02\n",
      "f3([0.99870267 0.98805617 0.98553371]) = 0.017497637837674974\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0, 5.1, 4.4])\n",
    "    xm = gradient_descend(f3, x0, 0.01, 1e-6, 50000) # ~ @30s\n",
    "    print(f'f3({xm}) = {f3(xm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Cas simple: **Donat un conjunt de punts (X, Y) 2D, podem trobar la recta que minimitza la distància entre aquesta i tots els punts?**\n",
    "\n",
    "Recordatori: \n",
    "\n",
    "Per tal de definir una recta necesitem dos punts $(x_0, y_0)$ i $(x_1, y_1)$. A partir d'aquests dos punts podem definir la recta com:\n",
    "\n",
    "$$y={\\frac {y_{1}-y_{0}}{x_{1}-x_{0}}} (x-x_{0}) +y_{0} = m \\cdot x + n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Lectura de les dades\n",
    "    df = pd.read_csv('data/slr05.csv', index_col=0)\n",
    "    display(df.head())\n",
    "\n",
    "    # Visualització de les dades\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.scatter(df['X'], df['Y'])\n",
    "    plt.xlabel('Fires per 1000 housing units')\n",
    "    plt.ylabel('Thefts per 1000 population')\n",
    "    plt.title('Fire and Theft in Chicago')\n",
    "\n",
    "    # Possibles linies\n",
    "    # Parametres plt.plot: (x0, x1), (y0, y1)\n",
    "    plt.plot((0, 40), (0, 140), color='#ff0000')\n",
    "    plt.plot((0, 40), (0, 120), color='#ff0000')\n",
    "    plt.plot((0, 40), (10, 110), color='#ff0000')\n",
    "    plt.plot((0, 40), (20, 70), color='#ff0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ara, donat que estem a 2D, podem visualitzar l'error, és a dir la distància respecte aquesta recta. Però tot i així és impossible que poguem visualitzar, d'entre les infinites línies possibles, la que s'ajusta millor a les dades.\n",
    "\n",
    "**Necessitem un algorisme que la trobi automàticament,** però primer visualitzem l'error de cadascuna de les anteriors rectes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for x1, y1, y0 in ((40, 140, 0), (40, 120, 0), (40, 110, 10), (40, 70, 20)):\n",
    "        plt.figure()\n",
    "        # Visualització de les dades\n",
    "        plt.scatter(df['X'], df['Y'])\n",
    "        plt.xlabel('Fires per 1000 housing units')\n",
    "        plt.ylabel('Thefts per 1000 population')\n",
    "        plt.title(f'Fire and Theft in Chicago, y = {y0} + x * {(y1 - y0) / x1:.2f}')\n",
    "\n",
    "        # Visualització de la recta\n",
    "        plt.plot((0, x1), (y0, y1), color='#ff0000')\n",
    "\n",
    "        # Definció de la recta\n",
    "        f = lambda x: (y1 - y0) / x1 * x + y0\n",
    "\n",
    "        # Visualització de l'error per cada punt\n",
    "        for _, (x, y) in df.iterrows():\n",
    "            y_pred = f(x)\n",
    "            plt.plot((x, x), (y, y_pred), color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Com definim un model que automàticament trobi aquesta recta?**\n",
    "\n",
    "Primer de tot, assumim que nosaltres tenim unes dades de les quals dispossem els valors de $x$ i els corresponents valors de $y$. Per exemple, les dades anteriors.\n",
    "\n",
    "1. Necessitem definir matemàticament una recta\n",
    "$$ r: \\hat{y} = m \\cdot x + n $$\n",
    "On $x$ és un punt que volem evaluar, $\\hat{y}$ és la imatge obtinguda en el punt $x$, i $m$, $n$ són els paràmetres que defineixen la recta. Idealment, voldríem trobar aquelles $m$, $n$ que, per tot punt $x$ tinguéssim una $\\hat{y}$ tal que $\\hat{y}=y$.\n",
    "\n",
    "2. Hem de mesurar l'error que estem cometent cada cop que provem una $m$, $n$ diferents (al que anomenarem *loss* del model). Per exemple, podríem definir l'error com a la distància entre el punt $y$ que hauria de ser i el $\\hat{y}$ que ens dona el model:\n",
    "$$\\mathbb{L} = \\sqrt{(y - \\hat{y})^2}$$\n",
    "\n",
    "3. Ara el que volem fer és trobar els valors de $m$, $n$ tal que $\\mathbb{L}=0$. Si parem atenció a la funció definida per $\\mathbb{L}$, veurem que efectivament $0$ és el mínim d'aquesta. Per tant, tot el que hem de fer és minimitzar la funció definida per\n",
    "$$\\mathbb{L} = \\sqrt{(y - (m \\cdot x + n))^2}$$\n",
    "\n",
    "Per simplificar una mica el problema, unirem les variables que volem optimitzar $m,n$ en una de sola: $w$. Per fer-ho, expandirem la fòrmula anterior a:\n",
    "$$\\tilde{x} = (1, x), w = (n, m)$$\n",
    "\n",
    "$$\\mathbb{L} = \\sqrt{(y - \\tilde{x} \\cdot w)^2}$$\n",
    "\n",
    "On $\\tilde{x}$ és un vector format per un $1$ i a continuació el valor de $x$ original. De forma que $\\tilde{x} \\cdot w = (1, x) \\cdot (n, m) = n + x\\cdot m = m\\cdot x + n$ (producte escalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"\n",
    "    Funció que donada el vector x crea el vector x', afegint una columna d'1's al davant.\n",
    "    Per exemple, si X és\n",
    "        [\n",
    "            [1, 2, 5, 1, 5, 7],\n",
    "            [9, 4, 2, 4, 6, 1],\n",
    "            ...\n",
    "            [5, 3, 1, 4, 5, 7]\n",
    "        ]\n",
    "    El retorn de la funció serà la matriu\n",
    "        [\n",
    "            [1, 1, 2, 5, 1, 5, 7],\n",
    "            [1, 9, 4, 2, 4, 6, 1],\n",
    "            ...\n",
    "            [1, 5, 3, 1, 4, 5, 7]\n",
    "        ]\n",
    "    \n",
    "    *Sense bucles*\n",
    "    \n",
    "    :param X: Matriu on cada fila és una dada i cada columna una característica\n",
    "    :return: Mateixa matriu amb una columna de 1s davant\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def linear_regression(x, w):\n",
    "    \"\"\"\n",
    "    Donat un punt i els parameters del model prediu el valor, implementant el càlcul de\n",
    "    la regressió lineal:\n",
    "            y' = w0 * x0 + w1 * x1 + ... + xp * wp\n",
    "            \n",
    "    *Sense bucles*\n",
    "    \n",
    "    :param x: Matriu de dades i features, on la primera columna son 1s, té per shape [NDades, NFeatures]\n",
    "    :param w: Matriu de paràmetres, té per shape [1, NFeatures]\n",
    "    :return: Vector de tamany [NDades] amb la predicció y' per a cada punt\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Funció que calcula la diferència entre la solució real i la predita mitjançant\n",
    "    distància euclidea\n",
    "        ||y_true - y_pred||\n",
    "        \n",
    "    :param y_true: Valor real de la Y\n",
    "    :param y_pred: Valor predit y' per a cada corresponent y\n",
    "    :return: Error de la predicció\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def model(w, x, y):\n",
    "    \"\"\"\n",
    "    Funció que genera el model que volem optimizar, calculant l'error d'una\n",
    "    predicció amb els pàrametres i valors passats\n",
    "    \n",
    "    :param w: Paràmetres del model\n",
    "    :param x: Dades del model\n",
    "    :param y: Valors a predir\n",
    "    :return: Error (loss) de les preddiccions\n",
    "    \"\"\"\n",
    "    return loss(y, linear_regression(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generem les dades que utilitzarem per entrenar el model (*X_fire*) i les seves etiquetes (*Y_fire*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_fire = df['X'].values[:, np.newaxis]\n",
    "    X_fire = add_bias(X_fire)\n",
    "    Y_fire = df['Y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per optimizar el model, necesitem initcialitzar els seus pesos amb valors random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parametres per defecte del model\n",
    "    w0 = np.random.normal(size=(1, X_fire.shape[1]))\n",
    "\n",
    "    # Optimització del model\n",
    "    wf_fire = gradient_descend(lambda w: model(w, X_fire, Y_fire), w0, 0.001, max_iter=20000, print_iters=1000) # ~ @5s\n",
    "    print(wf_fire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualització de la recta obtinguda amb el model i els errors de les seves prediccions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.figure()\n",
    "    # Visualització de les dades\n",
    "    plt.scatter(df['X'], df['Y'])\n",
    "    plt.xlabel('fires per 1000 housing units')\n",
    "    plt.ylabel('thefts per 1000 population')\n",
    "    plt.title('Fire and Theft in Chicago')\n",
    "\n",
    "    # Visualització de la recta\n",
    "    # Parametres plt.plot: (x0, x1), (y0, y1)\n",
    "    plt.plot((0, 40), (wf_fire[0, 0], linear_regression((1, 40), wf_fire)), color='#ff0000')\n",
    "\n",
    "    # Visualització de l'error a cada punt\n",
    "    for _, (x, y) in df.iterrows():\n",
    "        y_pred = linear_regression((1, x), wf_fire)\n",
    "        plt.plot((x, x), (y, y_pred), color='b', alpha=0.2)\n",
    "\n",
    "    y_fire_pred = linear_regression(X_fire, wf_fire)\n",
    "    print('Mean error: {:.2f}'.format(np.mean(np.abs(Y_fire - y_fire_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dades multidimensionals sobre les dades\n",
    "\n",
    "En l'apartat anterior tenim un sol valor de $x$ pel qual volíem obtenir un valor de $y$, però en la gran majoria de casos no serà així.\n",
    "\n",
    "En aquest apartat volem trobar una regressió lineal tal que $dim(X) = 6$, i per tant necessitem un total de 7 paràmetres ($dim(w) = 7$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Lectura de les dades\n",
    "    dg = pd.read_csv('data/mlr10.csv', index_col=0)\n",
    "    Y_pop = dg.values[:, 0]\n",
    "    X_pop = dg.values[:, 1:]\n",
    "    X_pop = add_bias(X_pop)\n",
    "    display(dg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parametres per defecte del model\n",
    "    w0 = np.random.normal(size=(1, X_pop.shape[1]))\n",
    "\n",
    "    # Optimització del model\n",
    "    wf_population = gradient_descend(lambda w: model(w, X_pop, Y_pop), w0, 0.001, max_iter=100000, print_iters=1000) # ~ @20s\n",
    "    print(wf_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Utilitzant els paràmetres apresos, obtenim la predicció per a cada punt\n",
    "    y_pop_pred = linear_regression(X_pop, wf_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Visualització de les dades originals i les prediccions\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.scatter(range(Y_pop.shape[0]), Y_pop, color='green', label = 'y')\n",
    "    plt.scatter(range(Y_pop.shape[0]), y_pop_pred, color='red', label = 'y_pred')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Total population')\n",
    "    plt.legend()\n",
    "\n",
    "    for x in range(X_pop.shape[0]):\n",
    "        plt.plot((x, x), (0, 20), '-.', color='gray', alpha=0.2)\n",
    "\n",
    "    print('Mean error: {:.2f}'.format(np.mean(np.abs(Y_pop - y_pop_pred))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
