{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimització - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estadística, la **regressió lineal** és un enfocament lineal per modelar la relació entre una resposta escalar (o variable dependent) i una o més variables explicatives (o variables independents).\n",
    "\n",
    "En regressió lineal, les relacions es modelen mitjançant funcions lineals on els paràmetres del model s’estimen a partir de les dades. \n",
    "Aquests models s'anomenen models lineals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suposem que treballem amb un conjunt de dades $ \\{y_{i}, x_{i_1}, \\ldots, x_{i_m} \\}_{i = 1}^{n}$ de $n$ unitats. \n",
    "\n",
    "Un **model de regressió lineal** assumeix que la relació entre la variable dependent $y$ i el vector $p$ dels regressors $x$ és lineal. \n",
    "Així, el model pren la forma:\n",
    "$${\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i_1}+\\cdots +\\beta _{p}x_{i_p},\\qquad i=1,\\ldots ,n,}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovem que autograd estigui instal·lat\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        import autograd\n",
    "    except:\n",
    "        print('No s\\'ha detectat autograd instal·lat')\n",
    "        if input('Vols instalar autograd? [s/n]').strip().lower() == 's':\n",
    "            !pip3 install --user -U autograd\n",
    "            !pip install --user -U autograd\n",
    "        else:\n",
    "            print('Abans de continuar instal·la autograd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import elementwise_grad as grad, value_and_grad\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from IPython.display import display\n",
    "    \n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es permet l'ús de cap llibreria o funció que no estigui ja importat, sigui aquí o més abaix en el notebook.\n",
    "\n",
    "Per Kaggle es permet l'ús d'altres llibreries/funcions sempre i quan únicament es facin servir pel processament de dades, i no pel model i/o predicció."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad\n",
    "\n",
    "En la part anterior, donada una funció havíem de definir també la funció que retorna el seu gradient.\n",
    "\n",
    "En aquesta pràctica, però, veurem que mitjançant l'ús de la llibreria **AutoGrad**, això no és necessari.\n",
    "\n",
    "Consideren les següents funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def f2(x):\n",
    "    x, y = x[0], x[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "def f3(x):\n",
    "    x, y, z = x[0], x[1], x[2]\n",
    "    return (100 * (y - x**2)**2 + (1 - x)**2) + (100 * (z - y**2)**2 + (1 - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donada la funció `f1`, podem fàcilment derivar el seu gradient, que és $2x$.\n",
    "\n",
    "Definim-lo manualment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f1_manual(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara, obtinguem la funció gradient a partir d'autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f1 = grad(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem comprovar que **les dues donen el mateix valor de gradient evaluat en un punt $x$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.asarray([1023.0])\n",
    "print('Calcul manual gradient: df1\\'(x0) = {}'.format(grad_f1_manual(x0)[0]))\n",
    "print('Autograd: df1\\'(x0) = {}'.format(grad_f1(x0)[0]))\n",
    "\n",
    "assert np.allclose(grad_f1_manual(x0), grad_f1(x0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També podem utilitzar `value_and_grad` per obtenir directament amb la **imatge de la funció i el seu gradient en un punt $x$**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_grad_f1 = value_and_grad(f1)\n",
    "print(val_grad_f1(x0))\n",
    "\n",
    "value, gradient = val_grad_f1(x0)\n",
    "assert np.allclose(value, f1(x0))\n",
    "assert np.allclose(gradient, grad_f1(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclús podem aplicar `grad` sobre un altre `grad` per obtenir la 2a derivada (o Hessià en multiples dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f1 = grad(f1) # df(x) = 2 * x\n",
    "grad2_f1 = grad(grad_f1) # d2f(x) = 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    plt.plot(x, f1(x), label = 'f(x)')\n",
    "    plt.plot(x, grad_f1(x), color='green', label = 'df(x)')\n",
    "    plt.plot(x, grad2_f1(x), color='orange', label = 'd2f(x)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descens del gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El primer pas serà implementar la mateixa funció `gradient_descend` que havieu fet en la part anterior, però ara mitjançant l'ús de la llibreria `autograd` i fent-la multidimensional.**\n",
    "\n",
    "Aquesta funció hauria d'executar sense problemes, i donar el mínim, per les tres funcions `f1`, `f2` i `f3` definides més adalt, de 1, 2 i 3 dimensions respectivament. Però, també haurà de funcionar per les següents parts de la pràctica, amb 100 o més dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per utilitzar aquesta llibreria cal tenir instalar el paquet tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def human_format(num):\n",
    "    \"\"\"\n",
    "    Funció auxiliar per formatar els nombres\n",
    "\n",
    "    100    -> 100.00\n",
    "    2100   -> 2.10K\n",
    "    342100 -> 342.10K\n",
    "    etc.    \n",
    "    \n",
    "    :param num: Nombre a formatar\n",
    "    :return: String amb el nombre formatat\n",
    "    \"\"\"\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.2f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
    "\n",
    "def gradient_descend(f, x0, alpha, eps=1e-6, max_iter=1000, print_iters=1000):\n",
    "    \"\"\"\n",
    "    Aquesta funció implementa l'algorisme de descens pel gradient, és a dir,\n",
    "    donat un punt inicial, la funció de la que calculem el gradient i el pas, \n",
    "    intenta trobar el mínim de la funció seguint el gradient en direcció oposada.\n",
    "    \n",
    "    Pel criteri d'aturada, considerarem si ||x^{k+1} - x^k|| < eps, és a dir\n",
    "    si la norma de la diferència és més petita que eps.\n",
    "    \n",
    "    Cada print_iters cal mostrar el resultat actual en la següent forma:\n",
    "        \"{}/{:.2f}\".format(human_format(itr), float(valor_de_f_en_x))\n",
    "    És dir, un missatge que inclogui la iteració i el valor de f en el punt\n",
    "    \n",
    "    :param f: Funció a minimitzar\n",
    "    :param x0: Punt inicial\n",
    "    :param alpha: Pas de cada iteració\n",
    "    :param eps: Moviment mínim realitzat abans de parar\n",
    "    :param max_epochs: Iteracions màximes a realitzar\n",
    "    :param print_iters: Numero d'iteracions per printar resultats\n",
    "    :return: La funció retornarà el punt on es troba el mínim\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busquem el mínim de cadascuna de les funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0])\n",
    "    xm = gradient_descend(f1, x0, 0.01, 1e-6, 10000) # ~ @1s\n",
    "    print(f'f1({xm}) = {f1(xm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0, 5.1])\n",
    "    xm = gradient_descend(f2, x0, 0.01, 1e-6, 10000) # ~ @1s\n",
    "    print(f'f2({xm}) = {f2(xm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x0 = np.array([4.0, 5.1, 4.4])\n",
    "    xm = gradient_descend(f3, x0, 0.01, 1e-6, 50000) # ~ @30s\n",
    "    print(f'f3({xm}) = {f3(xm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Cas simple: **Donat un conjunt de punts (X, Y) 2D, podem trobar la recta que minimitza la distància entre aquesta i tots els punts?**\n",
    "\n",
    "Recordatori: \n",
    "\n",
    "Per tal de definir una recta necesitem dos punts $(x_0, y_0)$ i $(x_1, y_1)$. A partir d'aquests dos punts podem definir la recta com:\n",
    "\n",
    "$$y={\\frac {y_{1}-y_{0}}{x_{1}-x_{0}}} (x-x_{0}) +y_{0} = m \\cdot x + n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Lectura de les dades\n",
    "    df = pd.read_csv('data/slr05.csv', index_col=0)\n",
    "    display(df.head())\n",
    "\n",
    "    # Visualització de les dades\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.scatter(df['X'], df['Y'])\n",
    "    plt.xlabel('Fires per 1000 housing units')\n",
    "    plt.ylabel('Thefts per 1000 population')\n",
    "    plt.title('Fire and Theft in Chicago')\n",
    "\n",
    "    # Possibles linies\n",
    "    # Parametres plt.plot: (x0, x1), (y0, y1)\n",
    "    plt.plot((0, 40), (0, 140), color='#ff0000')\n",
    "    plt.plot((0, 40), (0, 120), color='#ff0000')\n",
    "    plt.plot((0, 40), (10, 110), color='#ff0000')\n",
    "    plt.plot((0, 40), (20, 70), color='#ff0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ara, donat que estem a 2D, podem visualitzar l'error, és a dir la distància respecte aquesta recta. Però tot i així és impossible que poguem visualitzar, d'entre les infinites línies possibles, la que s'ajusta millor a les dades.\n",
    "\n",
    "**Necessitem un algorisme que la trobi automàticament,** però primer visualitzem l'error de cadascuna de les anteriors rectes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for x1, y1, y0 in ((40, 140, 0), (40, 120, 0), (40, 110, 10), (40, 70, 20)):\n",
    "        plt.figure()\n",
    "        # Visualització de les dades\n",
    "        plt.scatter(df['X'], df['Y'])\n",
    "        plt.xlabel('Fires per 1000 housing units')\n",
    "        plt.ylabel('Thefts per 1000 population')\n",
    "        plt.title(f'Fire and Theft in Chicago, y = {y0} + x * {(y1 - y0) / x1:.2f}')\n",
    "\n",
    "        # Visualització de la recta\n",
    "        plt.plot((0, x1), (y0, y1), color='#ff0000')\n",
    "\n",
    "        # Definció de la recta\n",
    "        f = lambda x: (y1 - y0) / x1 * x + y0\n",
    "\n",
    "        # Visualització de l'error per cada punt\n",
    "        for _, (x, y) in df.iterrows():\n",
    "            y_pred = f(x)\n",
    "            plt.plot((x, x), (y, y_pred), color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Com definim un model que automàticament trobi aquesta recta?**\n",
    "\n",
    "Primer de tot, assumim que nosaltres tenim unes dades de les quals dispossem els valors de $x$ i els corresponents valors de $y$. Per exemple, les dades anteriors.\n",
    "\n",
    "1. Necessitem definir matemàticament una recta\n",
    "$$ r: \\hat{y} = m \\cdot x + n $$\n",
    "On $x$ és un punt que volem evaluar, $\\hat{y}$ és la imatge obtinguda en el punt $x$, i $m$, $n$ són els paràmetres que defineixen la recta. Idealment, voldríem trobar aquelles $m$, $n$ que, per tot punt $x$ tinguéssim una $\\hat{y}$ tal que $\\hat{y}=y$.\n",
    "\n",
    "2. Hem de mesurar l'error que estem cometent cada cop que provem una $m$, $n$ diferents (al que anomenarem *loss* del model). Per exemple, podríem definir l'error com a la distància entre el punt $y$ que hauria de ser i el $\\hat{y}$ que ens dona el model:\n",
    "$$\\mathbb{L} = \\sqrt{(y - \\hat{y})^2}$$\n",
    "\n",
    "3. Ara el que volem fer és trobar els valors de $m$, $n$ tal que $\\mathbb{L}=0$. Si parem atenció a la funció definida per $\\mathbb{L}$, veurem que efectivament $0$ és el mínim d'aquesta. Per tant, tot el que hem de fer és minimitzar la funció definida per\n",
    "$$\\mathbb{L} = \\sqrt{(y - (m \\cdot x + n))^2}$$\n",
    "\n",
    "Per simplificar una mica el problema, unirem les variables que volem optimitzar $m,n$ en una de sola: $w$. Per fer-ho, expandirem la fòrmula anterior a:\n",
    "$$\\tilde{x} = (1, x), w = (n, m)$$\n",
    "\n",
    "$$\\mathbb{L} = \\sqrt{(y - \\tilde{x} \\cdot w)^2}$$\n",
    "\n",
    "On $\\tilde{x}$ és un vector format per un $1$ i a continuació el valor de $x$ original. De forma que $\\tilde{x} \\cdot w = (1, x) \\cdot (n, m) = n + x\\cdot m = m\\cdot x + n$ (producte escalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    \"\"\"\n",
    "    Funció que donada el vector x crea el vector x', afegint una columna d'1's al davant.\n",
    "    Per exemple, si X és\n",
    "        [\n",
    "            [1, 2, 5, 1, 5, 7],\n",
    "            [9, 4, 2, 4, 6, 1],\n",
    "            ...\n",
    "            [5, 3, 1, 4, 5, 7]\n",
    "        ]\n",
    "    El retorn de la funció serà la matriu\n",
    "        [\n",
    "            [1, 1, 2, 5, 1, 5, 7],\n",
    "            [1, 9, 4, 2, 4, 6, 1],\n",
    "            ...\n",
    "            [1, 5, 3, 1, 4, 5, 7]\n",
    "        ]\n",
    "    \n",
    "    *Sense bucles*\n",
    "    \n",
    "    :param X: Matriu on cada fila és una dada i cada columna una característica\n",
    "    :return: Mateixa matriu amb una columna de 1s davant\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def linear_regression(x, w):\n",
    "    \"\"\"\n",
    "    Donat un punt i els parameters del model prediu el valor, implementant el càlcul de\n",
    "    la regressió lineal:\n",
    "            y' = w0 * x0 + w1 * x1 + ... + xp * wp\n",
    "            \n",
    "    *Sense bucles*\n",
    "    \n",
    "    :param x: Matriu de dades i features, on la primera columna son 1s, té per shape [NDades, NFeatures]\n",
    "    :param w: Matriu de paràmetres, té per shape [1, NFeatures]\n",
    "    :return: Vector de tamany [NDades] amb la predicció y' per a cada punt\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Funció que calcula la diferència entre la solució real i la predita mitjançant\n",
    "    distància euclidea\n",
    "        ||y_true - y_pred||\n",
    "        \n",
    "    :param y_true: Valor real de la Y\n",
    "    :param y_pred: Valor predit y' per a cada corresponent y\n",
    "    :return: Error de la predicció\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def model(w, x, y):\n",
    "    \"\"\"\n",
    "    Funció que genera el model que volem optimizar, calculant l'error d'una\n",
    "    predicció amb els pàrametres i valors passats\n",
    "    \n",
    "    :param w: Paràmetres del model\n",
    "    :param x: Dades del model\n",
    "    :param y: Valors a predir\n",
    "    :return: Error (loss) de les preddiccions\n",
    "    \"\"\"\n",
    "    return loss(y, linear_regression(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generem les dades que utilitzarem per entrenar el model (*X_fire*) i les seves etiquetes (*Y_fire*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_fire = df['X'].values[:, np.newaxis]\n",
    "    X_fire = add_bias(X_fire)\n",
    "    Y_fire = df['Y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per optimizar el model, necesitem initcialitzar els seus pesos amb valors random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parametres per defecte del model\n",
    "    w0 = np.random.normal(size=(1, X_fire.shape[1]))\n",
    "\n",
    "    # Optimització del model\n",
    "    wf_fire = gradient_descend(lambda w: model(w, X_fire, Y_fire), w0, 0.001, max_iter=20000, print_iters=1000) # ~ @5s\n",
    "    print(wf_fire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualització de la recta obtinguda amb el model i els errors de les seves prediccions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    plt.figure()\n",
    "    # Visualització de les dades\n",
    "    plt.scatter(df['X'], df['Y'])\n",
    "    plt.xlabel('fires per 1000 housing units')\n",
    "    plt.ylabel('thefts per 1000 population')\n",
    "    plt.title('Fire and Theft in Chicago')\n",
    "\n",
    "    # Visualització de la recta\n",
    "    # Parametres plt.plot: (x0, x1), (y0, y1)\n",
    "    plt.plot((0, 40), (wf_fire[0, 0], linear_regression((1, 40), wf_fire)), color='#ff0000')\n",
    "\n",
    "    # Visualització de l'error a cada punt\n",
    "    for _, (x, y) in df.iterrows():\n",
    "        y_pred = linear_regression((1, x), wf_fire)\n",
    "        plt.plot((x, x), (y, y_pred), color='b', alpha=0.2)\n",
    "\n",
    "    y_fire_pred = linear_regression(X_fire, wf_fire)\n",
    "    print('Mean error: {:.2f}'.format(np.mean(np.abs(Y_fire - y_fire_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dades multidimensionals sobre les dades\n",
    "\n",
    "En l'apartat anterior tenim un sol valor de $x$ pel qual volíem obtenir un valor de $y$, però en la gran majoria de casos no serà així.\n",
    "\n",
    "En aquest apartat volem trobar una regressió lineal tal que $dim(X) = 6$, i per tant necessitem un total de 7 paràmetres ($dim(w) = 7$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Lectura de les dades\n",
    "    dg = pd.read_csv('data/mlr10.csv', index_col=0)\n",
    "    Y_pop = dg.values[:, 0]\n",
    "    X_pop = dg.values[:, 1:]\n",
    "    X_pop = add_bias(X_pop)\n",
    "    display(dg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parametres per defecte del model\n",
    "    w0 = np.random.normal(size=(1, X_pop.shape[1]))\n",
    "\n",
    "    # Optimització del model\n",
    "    wf_population = gradient_descend(lambda w: model(w, X_pop, Y_pop), w0, 0.001, max_iter=100000, print_iters=1000) # ~ @20s\n",
    "    print(wf_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Utilitzant els paràmetres apresos, obtenim la predicció per a cada punt\n",
    "    y_pop_pred = linear_regression(X_pop, wf_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Visualització de les dades originals i les prediccions\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.scatter(range(Y_pop.shape[0]), Y_pop, color='green', label = 'y')\n",
    "    plt.scatter(range(Y_pop.shape[0]), y_pop_pred, color='red', label = 'y_pred')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Total population')\n",
    "    plt.legend()\n",
    "\n",
    "    for x in range(X_pop.shape[0]):\n",
    "        plt.plot((x, x), (0, 20), '-.', color='gray', alpha=0.2)\n",
    "\n",
    "    print('Mean error: {:.2f}'.format(np.mean(np.abs(Y_pop - y_pop_pred))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
